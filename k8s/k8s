Configuring kubeconfig file

vi kubeconfig.yaml  
export KUBECONFIG=$PWD/kubeconfig.yaml

Or

mkdir .kube  
cp /Users/manojchaurasia/Downloads/kube-config.yml .kube/config
cat $HOME/.kube/config  

kubectl get pods --namespace=<namespace>
kubectl get deployment --namespace=<namespace>
kubectl describe pod <podname>    -n <namespace>
kubectl get poddisruptionbudgets  <poddisruptionbudgetname>  -n <namespace> -o yaml > pdb.yaml

kubectl logs <podname> -c <containername> -n otbl-dev
kubectl describe deployment <deployment-name> -n otbl-dev


kubectl get hpa -n <namespace>
kubectl get hpa <hpa-name> --watch -n <namespace>
kubectl describe hpa  <hpa-name> -n <namespace>

kubectl top pod --namespace=<namespace>
kubectl describe svc frontend
kubectl get servicemonitor  -n otbl-dev 
kubectl describe servicemonitor   <service-monitor-name> -n <namespace>
kubectl get svc -n <namespace>
kubectl get events
kubectl exec -it -n <namespace> <pod> -- /bin/bash 
kubectl get crds | grep longhorn.io
kubectl get crds | grep longhorn.io | awk '{print $1}' | xargs kubectl delete crd
kubectl apply -f <file>  --request-timeout=4s
kubectl get all -n otbl-dev -l app.kubernetes.io/instance="redis-cache"

To check if readiness and liveness probe working---->

https://ingresshosturl/actuator/health/readiness---> check readiness and liveness probe via ingress
https://ingresshosturl/actuator/health/liveness---> 


To check if prometheus service monitor working on actuator/prometheus endpoint----->

https://ingresshosturl/actuator/prometheus

curl https://ingresshosturl/actuator/prometheus

Service monitor-->

Add the following dependencies to your Gradle build file:

implementation 'org.springframework.boot:spring-boot-starter-actuator'
implementation 'io.micrometer:micrometer-registry-prometheus'

Configure the Prometheus registry by adding the following lines to your application.yml or application.properties file:

management.endpoints.web.exposure.include: prometheus
management.endpoint.prometheus.enabled: true
management.endpoint.health.probes.enabled: true

The metrics section specifies a set of tags to be added to all metrics generated by the application.

metrics:
  tags:
    application: application-name
    stack: dev


servicemonitor error --->

application---->fallback error while accesing  /acutator/prometheus endpoint-->
resolution change in application.yaml


application---->getting erorr ServiceMonitor "service-monitor-name" in namespace "" exists and cannot be imported into the current release: invalid ownership metadata; label validation error: missing key "app.kubernetes.io/managed-by": must be set to "Helm"; annotation validation error: missing key "meta.helm.sh/release-name": must be set to "app-helm"; annotation validation error: missing key "meta.helm.sh/release-namespace": must be set to "namespace"--> 
resolution delete servicemonitor

kubectl delete servicemonitor service-monitorname -n namespace

For spring boot application 

We have to update application.yml file for prometheus service monitoring and readiness and liveness probe and also for tracing configuration add metrics 

metrics:    tags:      application: app-name      stack: dev


Accessing k8s APIs---------->

kubectl config view

Signing and verifying image using cosign----------->

1)cosign generate key pair
2)Sign Container Images in the pipeline
cosign login  registry-address -u registry-user  -p registry-pass
https://github.com/sigstore/cosign/blob/main/doc/cosign_login.md

 # sign a container image with a key pair stored in a Kubernetes secret
  cosign sign --key k8s://[NAMESPACE]/[KEY] <IMAGE DIGEST/FQDN>
https://github.com/sigstore/cosign/blob/main/doc/cosign_sign.md


3)Observe the signed images in container repo
4)Create the cosign secret in kubernetes from the pipeline

kubectl create secret docker-registry my-docker-secret \
  --docker-username=your-docker-username \
  --docker-password=your-docker-password \
  --docker-email=your-docker-email \
  --docker-server=your-docker-server \
  --dry-run=client \
  -o jsonpath='{.data.\.dockerconfigjson}' > .dockerconfigjson

5)Build Cosign Docker Image (for Verification) 

FROM debian:buster-slim

# Set the cosign version
ARG COSIGN_VERSION=1.2.1

# Download the cosign binary
RUN apt-get update && apt-get install -y curl
RUN curl -LO https://github.com/sigstore/cosign/releases/download/v${COSIGN_VERSION}/cosign-linux-amd64 && \
    chmod +x cosign-linux-amd64 && \
    mv cosign-linux-amd64 /usr/local/bin/cosign


# Set the entrypoint
ENTRYPOINT ["cosign"]


6)Create a Service Account ,role, rolebinding for the image verification which having access on secrets and SA

creating a ServiceAccount named "cosign" and assigning it a Role with the ability to get secrets. The RoleBinding then associates the Role with the ServiceAccount, effectively granting the ServiceAccount the permissions defined by the Role.

7)Create initcontainer for the image verification
---use above build docker image
 # verify image with public key stored in a Kubernetes secret
  cosign verify --key k8s://[NAMESPACE]/[KEY] <IMAGE>
use created secret as volume so that docker image fqdn can be authenticated
imagePullPolicy: Always

apiVersion: v1
kind: Pod
metadata:
  name: pod-with-docker-secret
spec:
  serviceAccountName: your-service-account
  containers:
  - name: main-container
    image: your-main-image:your-tag
    # Add your main container configuration here
  
  initContainers:
  - name: fetch-cosign-image
    image: docker:latest
    command: ["sh", "-c", "docker login -u $DOCKER_USERNAME -p $DOCKER_PASSWORD $DOCKER_SERVER"]
    env:
    - name: DOCKER_USERNAME
      valueFrom:
        secretKeyRef:
          name: docker-secret
          key: .dockerconfigjson
    volumeMounts:
    - name: docker-config
      mountPath: /root/.docker
      
  # Add other containers and volumes here
  
volumes:
- name: docker-config
  secret:
    secretName: docker-secret
    items:
    - key: .dockerconfigjson
      path: .dockerconfigjson

-----------------------------------
check logs of initcontainer

add_pod_&_host_ip in kibana -----------------> add fulentbit as sidecar container and add its dependencies in build.gradle

 implementation "net.logstash.logback:logstash-logback-encoder"


Add below fields in logback-spring.xml

   <springProperty scope="context" name="pod_ip" source="POD_IP" />
    <springProperty scope="context" name="host_ip" source="HOST_IP" />


Add below fields in k8s deployment.yml

            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP   

---------------------------------------------



killall Google\ Chrome ----------

Delete pv:-

https://stackoverflow.com/questions/54629660/kubernetes-how-do-i-delete-pv-in-the-correct-manner

kubectl delete pv <pv_name> --grace-period=0 --force
kubectl patch pv <pv_name> -p '{"metadata": {"finalizers": null}}'
-------------------------
mkdir -p kubernetes
- if [ -f k8s/ingress.yml  ]; then envsubst "$(printf '${%s} ' $(env | cut -d'=' -f1))" < k8s/ingress.yml > kubernetes/ingress.yml; fi;

Here k8s if folder and ingress.yml if file in it We replacing all the env variables available in ingress.yml with actual value using envsubst




